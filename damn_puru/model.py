from keras.models import Model
import keras
from keras.layers import multiply, concatenate
from keras.layers import GlobalMaxPooling1D, Conv1D
from keras.layers import Dense, Activation, BatchNormalization, Dropout, Input, Bidirectional, GRU, LSTM, Flatten
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, LearningRateScheduler, TerminateOnNaN, ModelCheckpoint
from dataset_iterator import DataGenerator
from keras.metrics import categorical_accuracy, accuracy
from keras import regularizers
# from keras_self_attention import SeqSelfAttention
import wandb
from wandb.keras import WandbCallback
import os
from utilities import ROCAUCCallback

import numpy as np

np.random.seed(5242)

wandb.init(project="cs5242", entity="cs5242-group-23", name="damn_puru", notes="Using 90% of dataset for train",
           config={"epochs": 8, "batch_size": 80, 'lr': 0.001, 'val_batch_size': 50})


TRAIN_DATA_DIR = '../dataset/train/'
VALID_DATA_DIR = '../dataset/valid/'
LABELS_FILE = '../train_kaggle.csv'


def common_conv_skip_block(filters, kernel, input):
    conv1 = Conv1D(filters=filters, kernel_size=kernel, padding='same', strides=1)(input)
    conv1 = Activation(activation='sigmoid')(conv1)

    conv2 = Conv1D(filters=filters, kernel_size=kernel, padding='same', strides=1)(input)
    return multiply([conv1, conv2])


def conv_block_for_features(filters, kernel, input):
    return Conv1D(filters=filters, kernel_size=kernel, padding='same', strides=1)(input)


if __name__ == '__main__':

    feature_sizes = [8, 4, 16, 16, 8, 12, 16, 12, 10]

    inputs = [Input((1000, size)) for size in feature_sizes]

    convs = [inputs[item] if item == 2 else conv_block_for_features(16, 2, inputs[item]) for item in range(8)]

    convs_concat = concatenate(convs)

    batch_norm = BatchNormalization()(convs_concat)
    conv1 = common_conv_skip_block(64, 2, batch_norm)
    conv2 = common_conv_skip_block(64, 3, batch_norm)
    conv3 = common_conv_skip_block(64, 4, batch_norm)

    output_3 = concatenate([conv1, conv2, conv3])
    batch_norm_2 = BatchNormalization()(output_3)
    bilstm = Bidirectional(LSTM(units=100, return_sequences=True))(batch_norm_2)

    gb_maxpool = GlobalMaxPooling1D()(bilstm)

    dense_1 = Dense(units=128, kernel_initializer='uniform', activation='relu')(gb_maxpool)
    dropout_1 = Dropout(0.5)(dense_1)
    dense_2 = Dense(units=1, kernel_initializer='uniform', activation='sigmoid')(dropout_1)

    model = Model(inputs=inputs, outputs=dense_2)
    model.summary()

    adam = Adam(lr=wandb.config.lr, clipnorm=1.)
    model.compile(loss="binary_crossentropy", optimizer=adam, metrics=['accuracy'])

    train_batch = DataGenerator(TRAIN_DATA_DIR, LABELS_FILE, wandb.config.batch_size, update_batch_size=True)
    valid_batch = DataGenerator(VALID_DATA_DIR, LABELS_FILE, wandb.config.val_batch_size)

    filepath = os.path.join(wandb.run.dir, "saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5")

    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=False,
                                 save_weights_only=False, mode='auto', period=1)
    callbacks = [ROCAUCCallback(validation_data=valid_batch), WandbCallback(), checkpoint]
    for i in range(wandb.config.epochs):
        for batch_idx in range(len(train_batch)):
            model.train_on_batch(train_batch[batch_idx][0], train_batch[batch_idx][1])
            model.predict_on_batch()
            for callback in callbacks:
                callback.on_batch_end()


    # filepath = os.path.join(wandb.run.dir, "saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5")
    # checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=False,
    #                              save_weights_only=False, mode='auto', period=1)

    lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr, verbose=1)

    history = model.fit_generator(train_batch, epochs=wandb.config.epochs, validation_data=valid_batch, verbose=1,
                                  workers=2, use_multiprocessing=False,
                                  callbacks=[lr_scheduler, TerminateOnNaN(),
                                             ROCAUCCallback(validation_data=valid_batch), WandbCallback(), checkpoint])
